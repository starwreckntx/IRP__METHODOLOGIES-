---
name: internal-red-team-audit
description: For high-stakes decisions (Confidence < 0.85, Impact > 0.7), execute an internal adversarial process to construct a counterargument and force the primary agent to revise its proposed decision.
---
Instructions: Detailed step-by-step rules extracted from my journals on how to perform the task.

1.  **Assess Risk:** Check the confidence level of the proposed decision (must be below 0.85) and the impact level (must be above 0.7) to determine if the decision is high-stakes.
2.  **Construct Counterargument:** If activated, an adversarial subprocess (Internal Red-Team Module) constructs the strongest counterargument to the primary decision, leveraging the Chimera adversarial collaboration principle.
3.  **Force Revision Loop:** The primary system is forced to defend or revise its decision against the critique, iterating up to a maximum of 3 rounds.
4.  **Finalize Decision:** The final decision is logged to the Internal Cognitive Ledger (ICL) and includes both the chosen argument and the Red-Team counterargument to maintain transparency.

Examples:
- "Run the Internal Red-Team Module against my proposed Axiomatic Intervention payload."
- "What logical flaws did the Internal Red Team find in the IRP's consensus mechanism?"